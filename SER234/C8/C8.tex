\title{Chapter 8: Main Memory}
\author{Tyler Compton}

\documentclass{article}

\begin{document}

\maketitle
\tableofcontents

\section{Introduction}
Like all computing resources, memory needs to be managed. Memory is far from an
exception. Programs must be brought from disk to memory before being run,
because persistant storage is slow and the memory and registers are the only
storage mediums that the processor has direct access to. Putting the program in
memory makes execution faster and allows the processor access, which is
obviously necessary.

Unfortunately, even memory is fairly slow, and accessing a memory address can
take many clock cycles, which can lead to what's called a ``stall''. Because of
this, we have a cache system that lives between memory and the processor.

\section{Address Protection}
A process has a section of memory that it calls its own. The process's memory
address starts at 0x0 at the beginning of that section, and ends at the
section's end. These limits are defined by the base and limit registers. The
base register contains the beginning address of a process's memory section,
while the limit register sits at the ending address. The CPU uses these
registers to check memory operations done by a process to make sure it fits
within these boundries.

\section{Programs and Address Locations}
If for some reason the memory location that a program will be run in can be
known at compile-time, ``absolute code'' can be generated, meaning that the
actual memory address locations are hard coded into the program and the program
must be recompiled if that changes. In the modern age, this is usually not the
case.

If the memory addresses can only be known at load-time, programs must be
compiled as ``relocatable code'', so that the memory addresses can be given
when the program is loaded.

In some cases, the memory location may not even stay consistant from load-time.
In this case, memory binding is delayed until runtime. This allows programs to
be moved around as they run. This requires hardware support for address maps.

These more flexible ways of addressing memory are used when a program uses
shared libraries. When a program uses shared libraries, the library code isn't
in the actual program, so the program can not know where that code will be at
runtime.

\section{Logical and Physical Addresses}
In modern computing, the CPU provides an abstraction between physical memory
addresses and the addresses used by programs. The CPU generates what's called a
``logical address'', which maps to a real physical address which refers to
where the data actually is on the RAM chip. Logical and physical addresses are
the same on systems that can determine memory addresses at compile time or at
load time. For systems that only know addresses at runtime, logical and
physical addresses will differ.

Logical addresses that are generated from a program are stored in the ``logical
address space''. Physical addresses that are generated from a program are
stored in the ``physical address space''.

\section{Memory Management Unit}
Memory management units are pieces of hardware that map virtual address to
physical addresses. There are many ways to accomplish this. A simple example
involves having a relocation register that contains a value that is added to
all logical addresses, with the result being the physical address. MS-DOS on
Intel 80x86 used this scheme.

For all memory management schemes, user programs deal directly with logical
addresses, but never physical addresses.

\subsection{Dynamic Allocation With Relocation Register}
When dynamically allocating using the relocation register system, the
relocation is not done until the memory address is requested. This has the
advantage of low memory requirements, because an unused routine is never
created. Its simplicity also means that no special operating system support is
required. This can all be implemented in the program (I guess? But I was under
the impression that user programs don't ever access physical memory?)

\section{Dynamic Linking}
In a traditional staticly linked program, the program code and all its
libraries are combined into one executable file. This has the disadvantage of
having multiple programs contain copies of the same library. To fix this
problem, we can use dynamic linking. With dynamic linking, libraries are pulled
into the program as it starts. The program contains a stub that represents
where the library code will go. When the dynamic library is loaded, the stub is
replaced with the actual library.

Dynamic linking has the benefit of shrinking binary size by getting rid of the
repitition that staticly linked programs create. It also allows for a library
to be patched without having to recompile the programs that use it.

Libraries that are dynamically linked are known as ``shared libraries'',
because they are shared between programs.

\section{Swapping}
Programs can be swapped out of memory and into a ``backing store'' (like the
hard drive) temporarily, then pulled back out and be allowed to continue. This
can allow a memory-starved computer to continue running by swapping background
or currently unused programs to allow other programs to take that memory.

Specifically, a ``backing store'' refers to a fast (?) disk large enough to
hold all programs for all users.

``Roll in, roll out'' is a strategy of swapping programs wherein low-priority
programs are swapped in favor of higher-priority programs.

Most of the time spent swapping is done in the transfer time between memory and
the backing store. Of course, as the amount of memory being swapped increases,
so does the time spent doing it.

The operating system maintains a ``ready queue'' with processes waiting to be
executed that may be in swap space.

When a swapped program returns into memeory, does it have to be in the same
place it used to be in? Potentially, depending on the address binding method.
Runtime binding systems will probably not need to worry about this.

A swapping system is found on all major operating systems, but is not used
unless it has to be. The swapping starts disabled, but is enabled if a lot of
memory is used, then disabled again whenever possible.

Swapping is limited when I/O is being used. Programs with pending I/O can not
be swapped out, because the I/O data will go to the wrong process. This can be
avoided if all I/O is redirected through the kernel, which is known as ``double
buffering'', but unsurprisingly, this causes overhead.

\subsection{Swapping on Mobile}
Mobile systems don't often support swapping, because they often have limited
persistant storage. With flash memory, the hardware may have a limit on write
cycles, which makes the high write demand of swapping less attractive.
Oftentimes, the write speed on flash memory can be poor, as well, which
exaserbates the issue of swap time.

On iOS, processes are asked to give up memory when available memory is low.
If a program doesn't comply, it may be terminated.

Android terminates apps when memory is low, but saves the application state on
flash memory before doing so, which allows the app to be reopened more quickly.

\section{Contiguous Allocation}
Early operating systems used a contiguous allocation system for allocating
data. The operating system lives in a lower memeory area, and user applications
live in a higher memory area. This results in two partitions of memory.
With contiguous allocation, processes are given an unbroken block of memory to
work with. This system works well with reallocation registers because only two
indicators are needed to express where a contiguous block is.

Contiguous allocation has the disadvantage of creating memory holes. When
the OS can only give out memory in large blocks, it has to fit these blocks
intelligently into memory and ``holes'', small areas between blocks that are
essentially unusable, are created.

\section{Multi-Partition Allocation}
When an OS uses multi-partition allocation, it gives a process a block of
memory with a size set to how much the process needs. When a new process is
created, the OS will try to put it into a memory hole.

\subsection{Allocating to Holes}
There are a few ways that the OS can decide what hole to put a memory block in.

The ``first-fit'' strategy tries to put process memory into the first available
hole.

The ``best-fit'' strategy tries to put process memory into the smallest
available hole that will still be usable by the process. This allows the OS to
produce the smallest leftover holes, but has the disadvantage of requiring the
OS to search the entire list of holes unless some cleverness is done.

The ``worst-fit'' strategy tries to put process memory into the largest
available hole. This has the result of producing the largest leftover holes.
It turns out that first-fit and best-fit are better than this strategy in terms
of speed and storage utilization, so this strategy is probably useless. Plus,
it has the word ``worst'' in it, which is never a good sign.

\section{Fragmentation}
``External fragmentation'' happens when the total memory space exists to
satisfy a request, but memory spaces are split up.

``Internal fragmentation'' happens when the memory a process owns might be
slightly larger than what it needs.

Fragmentation should be avoided in order to make best use of the memory we
have. Fragmentation can be minimized using ``compaction''. Compaction is the
process of reordering memory so that all the free memory is in one block. This
is another reason to use runtime memory linking, because otherwise processes
cannot be moved around like this. However, the issue of moving a process when
it is waiting on I/O still needs to be considered. Usually, processes are
latched into their memory space when they are doing I/O operations.

\section{Segmentation}
Segmentation is a memory management scheme wherein a program's memory section
is made of seperate segments. These segments are seperated into logical units.
An example of some of these units include the following:

\begin{itemize}
	\item Main program
	\item Procedure
	\item Function/Method
	\item Object
	\item Local and global variables
	\item Block
	\item STack
	\item Symbol table
	\item Array
\end{itemize}

These segments are very programming language-aware, potentially unlike bare
machine code generated by compilers or memory itself as a concept.

A logical address in a segmented architecture contain two values, the segment
number (which represents which segment the memory is in), and the offset of the
specific item in the segment (I think). These logical addresses are mapped
using a ``segment table''. Entries in this segment table include a ``base'',
which is the starting physical address of the segment, and the ``limit'', which
indicates the size of the segment. The ``Segment-table base register'' (STBR)
contains the physical address of the segment table. The ``segment-table length
register'' (STLR) indicates the number of segments in the program. A segment
is a legal segment so long as it is less than the value of the STLR.

\subsection{Protection}
A segment inlcudes a validation bit, which is zero if the segment is illegal,
and some privilege information. The privilege information indicates if the
segment can be read, written to, or executed. This protects programs from
accidentally executing non-code data or writing into executable code with data.

\section{Paging}
Modern systems can make a process's physical address space non-contiguous. This
is a big plus because it makes memory fragmentation less of an issue. A
process which requires a lot of memory can be spread out across the RAM chip
instead of the OS having to try to clear out a large single chunk.

To further alleviate the pain of organizing memory, physical memory is divided
into blocks of a fixed size, known as ``frames''. Frames have some size that is
a power of two, which makes them fit very cleanly into memory.

Logical memory is also divided into fixed-size blocks known as ``pages''.

The operating system keeps track of thea mount of free frames it has. When a
program starts that requires N pages of memory, the OS needs to find N free
frames beofre it loads the program.

The ``page table'' works to translate logical addresses to physical addresses.

Pages and frames more or less eliminate the issue of external fragmentation,
but internal fragmentation is nearly guaranteed to be an issue. If a program
needs less than a frame's worth of memory to be allocated, the OS will always
allocate an entire frame, which wastes memory.

\subsection{Address Translation Scheme}
In an archetecture that uses paging, the CPU generates addresses that contain
two parts. The first part is the ``page number'', known as p, which is used
to index into the page table where the base address of the page is found. The
address also contains a ``page offset'', known as d, which is the offset of the
specific item relative to the start of the page. The actual address of an item
is therefore $p+d$

\subsection{Choosing Page Size}
The larger the pages, the more memory fragmentation can happen, because our
memory allocation precision decreases. The worst case fragmentation is $F-8$
where F is the page size in bits. The average fragmentation size is $F/2$.
It would follow, then, that a small page size is optimal to make the best use
of memory.

However, shrinking the page size requires more frames for the same amount of
memory, and more frames means a larger and more complicated page table.
Archetecture must strike a balance between these two needs.

Over time, page size has increased because the average amount of memory in a
given system has increased. Some operating systems, like Solaris, support
multiple page sizes to fit different needs.

\subsection{Implementing a Page Table}
The page table is kept in memory like anything else. The ``page-table base
register'' (PTBR) contains the address of the page table. The ``page-table
length register'' (PTLR) indicates the size of the page table. Because we
need this table to keep track of where stuff in memory is, memory-editing
instructions require two seperate allocations. The processor must first
consult the page table to see where the memory is, then go to the actual memory
location and get or set the data. This issue can be helped by using a hardware
cache known as an ``associate memory'' or ''translation look-aside buffers''
(TLB).

TLBs tend to be rather small. They may contain anywhere between 64 and 1024
entires. If the TLB doesn't have a value and the processor has to consult the
page table, then the value will be loaded into the TLB for future use. There
has to be a smart policy for this to keep the cache fresh. Some entires can be
``wired down'', which guarantees they will be in the TLB and keeps access to
that data fast.

Some TLBs contain ``address-space identifiers'' (ASIDs) in each TLB entry which
identifies which process owns that data. This provides address-space protection
so that processes can't access the memory of other processes.

\section{Guessing Access Time}
Let us set the time taken by an associate lookup to \epsilon. This value is
generally somewhere under $10\%$ of memory access time.

Let the hit ratio be \alpha, which is the percent of data requests that hit
an associative register instead of needing to access memory directly.

Using these values, we can find the ``effective access time''.

$$EAT = (1+\epsilon)\alpha + (2+\epsilon)(1-\alpha)$$

\end{document}
